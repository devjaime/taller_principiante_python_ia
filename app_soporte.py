import streamlit as st
from langchain_community.llms import Ollama
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# Configuraci贸n inicial del modelo de lenguaje
llm = Ollama(model="llama3.1:latest")


def main():
    # T铆tulo principal de la aplicaci贸n
    st.title("Chat con Llama 3.1 para Soporte 24/7")

    # Secci贸n de configuraci贸n del asistente virtual
    st.sidebar.header("Configuraci贸n del Asistente")
    bot_name = st.sidebar.text_input("Nombre del asistente virtual:", value="Bot")
    prompt_default = f"""Eres un asistente virtual llamado {bot_name}, especializado en soporte t茅cnico de nivel 1. Respondes de forma simple y clara, y realizas preguntas relevantes para recopilar m谩s detalles del problema del usuario. Tambi茅n haces preguntas b谩sicas para conocer al usuario."""
    bot_description = st.sidebar.text_area("Descripci贸n del asistente virtual:", value=prompt_default)

    # Inicializaci贸n del historial de chat
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []

    # Plantilla de prompt para el asistente
    prompt_template = ChatPromptTemplate.from_messages(
        [
            ("system", bot_description),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    chain = prompt_template | llm

    # Entrada del usuario
    st.subheader("Interacci贸n")
    user_input = st.text_input("Escribe tu pregunta o consulta:", key="user_input")

    # Bot贸n de env铆o
    if st.button("Enviar"):
        if user_input.lower() == "adios":
            st.write(" 隆Gracias por usar el asistente! Hasta luego.")
            st.stop()
        else:
            # Generaci贸n de respuesta del asistente
            response = chain.invoke({
                "input": user_input,
                "chat_history": st.session_state["chat_history"]
            })
            st.session_state["chat_history"].append(HumanMessage(content=user_input))
            st.session_state["chat_history"].append(AIMessage(content=response))

    # Visualizaci贸n del historial de chat
    st.subheader("Historial de Chat")
    for msg in st.session_state["chat_history"]:
        if isinstance(msg, HumanMessage):
            st.markdown(f"**さ Humano:** {msg.content}")
        elif isinstance(msg, AIMessage):
            st.markdown(f"** {bot_name}:** {msg.content}")

    # Gr谩ficos para analizar el uso del chat
    st.sidebar.subheader("Estad铆sticas del Chat")
    total_messages = len(st.session_state["chat_history"])
    human_messages = len([msg for msg in st.session_state["chat_history"] if isinstance(msg, HumanMessage)])
    ai_messages = len([msg for msg in st.session_state["chat_history"] if isinstance(msg, AIMessage)])

    st.sidebar.metric("Mensajes Totales", total_messages)
    st.sidebar.metric("Mensajes Humanos", human_messages)
    st.sidebar.metric("Respuestas del Bot", ai_messages)

    # Gr谩fico de proporci贸n de mensajes
    if total_messages > 0:
        st.sidebar.subheader("Distribuci贸n de Mensajes")
        st.sidebar.bar_chart({
            "Tipo de Mensaje": ["Humano", "Bot"],
            "Cantidad": [human_messages, ai_messages],
        })

    # Notas adicionales para personalizaci贸n futura
    st.sidebar.info("Puedes personalizar la descripci贸n y el nombre del asistente para adaptarlo a tus necesidades.")


# Punto de entrada principal
if __name__ == '__main__':
    main()
